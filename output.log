/DATA/raj13/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
/DATA/raj13/.local/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
<bound method Module.children of Sequential(
  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (5): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (6): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (7): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (8): AdaptiveAvgPool2d(output_size=(1, 1))
)>
cuda
0 starting
Epoch [0/10] Batch 0/469 \Loss D: 0.6930, loss G: 0.7052
Epoch [0/10] Batch 100/469 \Loss D: 0.5236, loss G: 0.8973
Epoch [0/10] Batch 200/469 \Loss D: 0.3368, loss G: 1.2902
Epoch [0/10] Batch 300/469 \Loss D: 0.1424, loss G: 2.0823
Epoch [0/10] Batch 400/469 \Loss D: 0.0630, loss G: 2.8712
1 starting
Epoch [1/10] Batch 0/469 \Loss D: 0.0407, loss G: 3.2988
Epoch [1/10] Batch 100/469 \Loss D: 0.0244, loss G: 3.7980
Epoch [1/10] Batch 200/469 \Loss D: 0.6917, loss G: 0.6837
Epoch [1/10] Batch 300/469 \Loss D: 0.7189, loss G: 0.6503
Epoch [1/10] Batch 400/469 \Loss D: 0.4094, loss G: 1.4973
2 starting
Epoch [2/10] Batch 0/469 \Loss D: 0.1841, loss G: 2.2855
Epoch [2/10] Batch 100/469 \Loss D: 0.2456, loss G: 1.6305
Epoch [2/10] Batch 200/469 \Loss D: 0.0553, loss G: 3.6129
Epoch [2/10] Batch 300/469 \Loss D: 0.1940, loss G: 2.8099
Epoch [2/10] Batch 400/469 \Loss D: 0.7471, loss G: 0.5916
3 starting
Epoch [3/10] Batch 0/469 \Loss D: 0.0893, loss G: 3.3765
Epoch [3/10] Batch 100/469 \Loss D: 0.1496, loss G: 2.2877
Epoch [3/10] Batch 200/469 \Loss D: 0.1509, loss G: 0.9642
Epoch [3/10] Batch 300/469 \Loss D: 0.0390, loss G: 3.0606
Epoch [3/10] Batch 400/469 \Loss D: 0.1897, loss G: 2.9684
4 starting
Epoch [4/10] Batch 0/469 \Loss D: 0.0683, loss G: 3.1845
Epoch [4/10] Batch 100/469 \Loss D: 0.1183, loss G: 4.0897
Epoch [4/10] Batch 200/469 \Loss D: 0.0985, loss G: 2.3775
Epoch [4/10] Batch 300/469 \Loss D: 0.0614, loss G: 2.9417
Epoch [4/10] Batch 400/469 \Loss D: 0.0786, loss G: 2.5567
5 starting
Epoch [5/10] Batch 0/469 \Loss D: 0.4452, loss G: 1.1272
Epoch [5/10] Batch 100/469 \Loss D: 0.0440, loss G: 2.7148
Epoch [5/10] Batch 200/469 \Loss D: 0.0083, loss G: 4.3940
Epoch [5/10] Batch 300/469 \Loss D: 0.6170, loss G: 1.2520
Epoch [5/10] Batch 400/469 \Loss D: 0.6095, loss G: 1.0344
6 starting
Epoch [6/10] Batch 0/469 \Loss D: 0.8020, loss G: 0.3120
Epoch [6/10] Batch 100/469 \Loss D: 0.1622, loss G: 1.7353
Epoch [6/10] Batch 200/469 \Loss D: 0.3566, loss G: 0.6558
Epoch [6/10] Batch 300/469 \Loss D: 0.0952, loss G: 2.8990
Epoch [6/10] Batch 400/469 \Loss D: 0.0771, loss G: 2.8315
7 starting
Epoch [7/10] Batch 0/469 \Loss D: 0.1034, loss G: 3.6315
Epoch [7/10] Batch 100/469 \Loss D: 0.1984, loss G: 4.3050
Epoch [7/10] Batch 200/469 \Loss D: 0.0573, loss G: 2.9154
Epoch [7/10] Batch 300/469 \Loss D: 0.3820, loss G: 3.5167
Epoch [7/10] Batch 400/469 \Loss D: 0.0163, loss G: 4.0801
8 starting
Epoch [8/10] Batch 0/469 \Loss D: 0.2417, loss G: 4.4321
Epoch [8/10] Batch 100/469 \Loss D: 0.0215, loss G: 3.8432
Epoch [8/10] Batch 200/469 \Loss D: 0.2619, loss G: 3.9767
Epoch [8/10] Batch 300/469 \Loss D: 0.1056, loss G: 4.2343
Epoch [8/10] Batch 400/469 \Loss D: 0.0867, loss G: 4.4502
9 starting
Epoch [9/10] Batch 0/469 \Loss D: 0.0136, loss G: 4.3635
Epoch [9/10] Batch 100/469 \Loss D: 0.0130, loss G: 4.3780
Epoch [9/10] Batch 200/469 \Loss D: 0.3706, loss G: 1.2449
Epoch [9/10] Batch 300/469 \Loss D: 0.0389, loss G: 3.3043
Epoch [9/10] Batch 400/469 \Loss D: 0.0273, loss G: 3.6036
