# -*- coding: utf-8 -*-
"""pxp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104e1p5Q6lWv1yPS0umf0hyKYrvabl26x
"""

import torch
import torch.nn as nn



class Block(nn.Module):
  def __init__(self,inc,out,stride):
    super().__init__()
    self.conv=nn.Sequential(nn.Conv2d(inc,out,4,stride,bias=False,padding_mode="reflect"),
                            nn.BatchNorm2d(out),
                            nn.LeakyReLU(0.2), 
                            )
  def forward(self,x):
    x=self.conv(x)
    return x

from torch.nn.modules.activation import LeakyReLU
class Discriminator(nn.Module):
  def __init__(self,inc=3,f=[64,128,256,512]):
    super().__init__()
    self.pnet=nn.Sequential(
        nn.Conv2d(inc*2,f[0],kernel_size=4,stride=2,padding=1,padding_mode="reflect"),
        nn.LeakyReLU(0.2),
    )
    layers=[]
    for i in range(1,len(f)):
      s=2
      if(f[i]==f[-1]):
        s=1
      layers.append(Block(f[i-1],f[i],s))
    layers.append(nn.Conv2d(f[-1],1,kernel_size=4,stride=1,padding=1,padding_mode="reflect"))
    self.net=nn.Sequential(*layers)


    #print(layers)


  def forward(self,x,y):
      x=torch.cat([x,y],dim=1)
      x=self.pnet(x)
      x=self.net(x)
      return x



class Block_gen(nn.Module):
    def __init__(self, inc, out, down=True, act="relu", use_dropout=False):
        super(Block_gen, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(inc, out, 4, 2, 1, bias=False, padding_mode="reflect")
            if down
            else nn.ConvTranspose2d(inc, out, 4, 2, 1, bias=False),
            nn.BatchNorm2d(out),
            nn.ReLU() if act == "relu" else nn.LeakyReLU(0.2),
        )

        self.use_dropout = use_dropout
        self.dropout = nn.Dropout(0.5)
        self.down = down

    def forward(self, x):
        x = self.conv(x)
        return self.dropout(x) if self.use_dropout else x
        return x

class Generator(nn.Module):
 def __init__(self,inc=3,f=64):
    super().__init__()
    self.block1=nn.Sequential(
        nn.Conv2d(inc,f,4,2,1,padding_mode="reflect"),
        nn.LeakyReLU(0.2),
    )
    self.dn1=Block_gen(f,f*2,down=True,act="leakyrelu",use_dropout=False)
    self.dn2=Block_gen(f*2,f*4,down=True,act="leakyrelu",use_dropout=False)
    self.dn3=Block_gen(f*4,f*8,down=True,act="leakyrelu",use_dropout=False)
    self.dn4=Block_gen(f*8,f*8,down=True,act="leakyrelu",use_dropout=False)
    self.dn5=Block_gen(f*8,f*8,down=True,act="leakyrelu",use_dropout=False)
    self.dn6=Block_gen(f*8,f*8,down=True,act="leakyrelu",use_dropout=False)

    self.bn=nn.Sequential(
        nn.Conv2d(f*8,f*8,4,2,1,padding_mode="reflect"),
        nn.ReLU(),

    )
    self.u1=Block_gen(f*8,f*8,down=False,use_dropout=True)
    self.u2=Block_gen(f*8*2,f*8,down=False,use_dropout=True)
    self.u3=Block_gen(f*8*2,f*8,down=False,use_dropout=True)
    self.u4=Block_gen(f*8*2,f*8,down=False,use_dropout=False)
    self.u5=Block_gen(f*8*2,f*4,down=False,use_dropout=False)
    self.u6=Block_gen(f*4*2,f*2,down=False,use_dropout=False)
    self.u7=Block_gen(f*2*2,f,down=False,use_dropout=False)

    self.block2=nn.Sequential(
        nn.ConvTranspose2d(f*2,inc,kernel_size=4,stride=2,padding=1),
        nn.Tanh()
    )
 def forward(self,x):
        d1 = self.block1(x)
        d2 = self.dn1(d1)
        d3 = self.dn2(d2)
        d4 = self.dn3(d3)
        d5 = self.dn4(d4)
        d6 = self.dn5(d5)
        d7 = self.dn6(d6)
        bottleneck = self.bn(d7)
        up1 = self.u1(bottleneck)
        up2 = self.u2(torch.cat([up1, d7], 1))
        up3 = self.u3(torch.cat([up2, d6], 1))
        up4 = self.u4(torch.cat([up3, d5], 1))
        up5 = self.u5(torch.cat([up4, d4], 1))
        up6 = self.u6(torch.cat([up5, d3], 1))
        up7 = self.u7(torch.cat([up6, d2], 1))
        return self.block2(torch.cat([up7, d1], 1))



